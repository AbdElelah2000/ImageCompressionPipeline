# -*- coding: utf-8 -*-
"""4x_downsample_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zm4AfzornxYTMAqmYNnK2mdtcxChk7_H
"""

#!wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip
!unzip /content/drive/MyDrive/training/DIV2K_train_HR.zip
from tensorflow.keras.layers import *
from tensorflow.keras.models import *
from glob import glob
import cv2
import numpy as np
from tqdm import tqdm
from keras.layers import MaxPooling2D


# Define the conversion matrices
Conversion_mat_RGB2YUV = np.array([[0.299, 0.587, 0.114], [0.5, -0.41869, -0.08131], [-0.16874, -0.33126, 0.5]])
Conversion_mat_YUV2RGB = np.array([[1.000, 0.000, 1.13983], [1.000, -0.39465, -0.58060], [1.000, 2.03211, 0.000]])

# A Function that converts rgb image to yuv image
def convert_RGB2YUV(rgb_image):
    yuv_image = np.zeros_like(rgb_image)
    yuv_image[:,:,0] = np.dot(rgb_image, Conversion_mat_RGB2YUV[0])
    yuv_image[:,:,1] = np.dot(rgb_image, Conversion_mat_RGB2YUV[1])
    yuv_image[:,:,2] = np.dot(rgb_image, Conversion_mat_RGB2YUV[2])
    image_rep = yuv_image[:,:,1:] + 128.0
    # 8-bit representation of the image
    yuv_image[:,:,1:] = image_rep.astype(np.uint8)

    return yuv_image


# Define the conversion matrices
Conversion_mat_YUV2BGR = np.array([[1.000, 0.000, 1.403], [1.000, -0.34413, -0.71414], [1.000, 1.773, 0.000]])
Conversion_mat_RGB2BGR = np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0]])

# A Function that converts yuv image to bgr image
def convert_YUV2RGB(yuv_image):
    yuv_image = yuv_image.astype(np.float32)
    yuv_image[:,:,1:] = yuv_image[:,:,1:] - 128.0
    rgb_image = np.dot(yuv_image, Conversion_mat_YUV2RGB.T)
    # Convert from RGB to BGR color space This is because we need it in the BGR order to show the image correctly with openCV
    bgr_image = np.dot(rgb_image, Conversion_mat_RGB2BGR.T)
    # 8-bit representation of the image
    bgr_image = clip_values(bgr_image, 0, 255).astype(np.uint8)
    return bgr_image

# A function to downsample the YUV image
def YUV_downsampling(image):
    # Get the YUV channels
    Y, U, V = split_image_channels(image)

    # Downsample the Y channel by a factor of 2
    Y_downsampled_channel = downsampling_average_kernel(Y, 2)

    # Downsample the U and V channels by a factor of 4
    U_downsampled_channel = downsampling_average_kernel(U, 4)
    V_downsampled_channel = downsampling_average_kernel(V, 4)

    return Y_downsampled_channel, U_downsampled_channel, V_downsampled_channel

# implementation of a simple average pooling kernel method
def downsampling_average_kernel(input_channel, downsample_factor):
    # Define the downsampling kernel
    ds_kernel = np.ones((downsample_factor, downsample_factor)) / (downsample_factor * downsample_factor)

    # Pad the input channel
    input_height, input_width = input_channel.shape
    padded_height = input_height + downsample_factor - (input_height % downsample_factor)
    padded_width = input_width + downsample_factor - (input_width % downsample_factor)
    padded_channel = np.zeros((padded_height, padded_width), dtype=input_channel.dtype)
    padded_channel[:input_height, :input_width] = input_channel

    # Apply the downsampling kernel to the padded channel
    downsampled_channel = np.zeros((padded_height // downsample_factor, padded_width // downsample_factor), dtype=input_channel.dtype)
    for i in range(0, padded_height, downsample_factor):
        for j in range(0, padded_width, downsample_factor):
            window = padded_channel[i:i+downsample_factor, j:j+downsample_factor]
            downsampled_channel[i//downsample_factor, j//downsample_factor] = sum_array(ds_kernel * window)

    # Remove the padding
    downsampled_channel = downsampled_channel[:input_height//downsample_factor, :input_width//downsample_factor]

    return downsampled_channel
  
# implementation of split
def split_image_channels(image):
    b_ch = image[:,:,0]
    g_ch = image[:,:,1]
    r_ch = image[:,:,2]
    return b_ch, g_ch, r_ch

# implementation of clip
def clip_values(array, min, max):
    clip = np.copy(array)
    # clip values
    clip[clip < min] = min
    clip[clip > max] = max
    return clip

# implementation of sum
def sum_array(array):
    sum = 0
    for i in range(array.shape[0]):
        for j in range(array.shape[1]):
            sum += array[i, j]
    return sum



def CNN_design():
    input = Input(shape=(None, None, 1))
    x = Conv2D(32, 3, activation=None, padding='same')(input)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv2D(64, 3, activation=None, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv2D(128, 3, activation=None, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = MaxPooling2D(pool_size=(2, 2))(x)
    x = MaxPooling2D(pool_size=(2, 2))(x)

    x = Conv2D(64, 3, activation=None, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv2D(32, 3, activation=None, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv2D(1, 3, activation=None, padding='same')(x)
    x = Activation('tanh')(x)
    x = x*127.5 + 127.5

    model = Model([input], x)
    model.summary()
    return model

def pull_database():
    x = []
    y = []
    for img_dir in tqdm(glob('/content/DIV2K_train_HR/*.png')):
      img = cv2.imread(img_dir)
      img_yuv = convert_RGB2YUV(img)
      u_channel = img_yuv[:,:,1]
      y_out = cv2.resize(u_channel, (256, 256), interpolation = cv2.INTER_AREA)
      y_in = cv2.resize(y_out, None, fx=4, fy=4, interpolation = cv2.INTER_AREA)
      x.append(y_in)
      y.append(y_out)
    x = np.array(x)
    y = np.array(y)

    return x,y

model = CNN_design()

x,y = pull_database()
print(x.shape, y.shape)

import matplotlib.pyplot as plt

plt.subplot(211)
plt.imshow(x[0], cmap='gray')
plt.subplot(212)
plt.imshow(y[0], cmap='gray')
plt.show()

from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping

X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state = 7623)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
loss = 'mse'
model.compile(loss='mse', optimizer=optimizer)

save_model_callback = tf.keras.callbacks.ModelCheckpoint(
    'drive/MyDrive/training/model/4x_downsample_model.h5',
    monitor = 'val_loss',
    verbose=1,
    save_best_only=True,
    mode='min',
    save_freq='epoch'
)
tbCallBack = tf.keras.callbacks.TensorBoard(log_dir='./drive/MyDrive/training/graph/4x_downsample/', histogram_freq=0, write_graph=True, write_images=True)
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)

batch_size = 4
epochs = 100
model.fit(X_train, y_train, validation_data=( X_val,y_val), batch_size=batch_size, epochs=100, validation_split=0.1, callbacks=[tbCallBack, save_model_callback, early_stopping_callback])

train_model = load_model('drive/MyDrive/training/model/4x_downsample_model.h5')
img = cv2.imread("drive/MyDrive/training/test_image.png")

img_yuv = convert_RGB2YUV(img)
U_input = img_yuv[:,:,1]
U_pred = np.expand_dims(U_input,axis=0)
U_downsampled = train_model.predict(U_pred)

print("input:", img_yuv[:,:,1].shape)
print("downsampled:",U_downsampled.shape)

plt.subplot(211)
plt.imshow(img_yuv[:,:,1], cmap='gray')
plt.subplot(212)
plt.imshow(U_downsampled[0], cmap='gray')
plt.show()