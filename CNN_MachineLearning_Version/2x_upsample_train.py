# -*- coding: utf-8 -*-
"""2x_upsample_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_bXf3n8z2FyotgmzfyLRIMxq9lRJuVr2
"""

#!wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip
!unzip /content/drive/MyDrive/training/DIV2K_train_HR.zip
from tensorflow.keras.layers import *
from tensorflow.keras.models import *
from glob import glob
import cv2
import numpy as np
from tqdm import tqdm



# A function to downsample the YUV image
def YUV_downsampling(image):
    # Get the YUV channels
    Y, U, V = split_image_channels(image)

    # Downsample the Y channel by a factor of 2
    Y_downsampled_channel = downsampling_average_kernel(Y, 2)

    # Downsample the U and V channels by a factor of 4
    U_downsampled_channel = downsampling_average_kernel(U, 4)
    V_downsampled_channel = downsampling_average_kernel(V, 4)

    return Y_downsampled_channel, U_downsampled_channel, V_downsampled_channel

# implementation of a simple average pooling kernel method
def downsampling_average_kernel(input_channel, downsample_factor):
    # Define the downsampling kernel
    ds_kernel = np.ones((downsample_factor, downsample_factor)) / (downsample_factor * downsample_factor)

    # Pad the input channel
    input_height, input_width = input_channel.shape
    padded_height = input_height + downsample_factor - (input_height % downsample_factor)
    padded_width = input_width + downsample_factor - (input_width % downsample_factor)
    padded_channel = np.zeros((padded_height, padded_width), dtype=input_channel.dtype)
    padded_channel[:input_height, :input_width] = input_channel

    # Apply the downsampling kernel to the padded channel
    downsampled_channel = np.zeros((padded_height // downsample_factor, padded_width // downsample_factor), dtype=input_channel.dtype)
    for i in range(0, padded_height, downsample_factor):
        for j in range(0, padded_width, downsample_factor):
            window = padded_channel[i:i+downsample_factor, j:j+downsample_factor]
            downsampled_channel[i//downsample_factor, j//downsample_factor] = sum_array(ds_kernel * window)

    # Remove the padding
    downsampled_channel = downsampled_channel[:input_height//downsample_factor, :input_width//downsample_factor]

    return downsampled_channel

# Define the conversion matrices
Conversion_mat_RGB2YUV = np.array([[0.299, 0.587, 0.114], [0.5, -0.41869, -0.08131], [-0.16874, -0.33126, 0.5]])
Conversion_mat_YUV2RGB = np.array([[1.000, 0.000, 1.13983], [1.000, -0.39465, -0.58060], [1.000, 2.03211, 0.000]])

# A Function that converts rgb image to yuv image
def convert_RGB2YUV(rgb_image):
    yuv_image = np.zeros_like(rgb_image)
    yuv_image[:,:,0] = np.dot(rgb_image, Conversion_mat_RGB2YUV[0])
    yuv_image[:,:,1] = np.dot(rgb_image, Conversion_mat_RGB2YUV[1])
    yuv_image[:,:,2] = np.dot(rgb_image, Conversion_mat_RGB2YUV[2])
    image_rep = yuv_image[:,:,1:] + 128.0
    # 8-bit representation of the image
    yuv_image[:,:,1:] = image_rep.astype(np.uint8)

    return yuv_image


# Define the conversion matrices
Conversion_mat_YUV2BGR = np.array([[1.000, 0.000, 1.403], [1.000, -0.34413, -0.71414], [1.000, 1.773, 0.000]])
Conversion_mat_RGB2BGR = np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0]])

# A Function that converts yuv image to bgr image
def convert_YUV2RGB(yuv_image):
    yuv_image = yuv_image.astype(np.float32)
    yuv_image[:,:,1:] = yuv_image[:,:,1:] - 128.0
    rgb_image = np.dot(yuv_image, Conversion_mat_YUV2RGB.T)
    # Convert from RGB to BGR color space This is because we need it in the BGR order to show the image correctly with openCV
    bgr_image = np.dot(rgb_image, Conversion_mat_RGB2BGR.T)
    # 8-bit representation of the image
    bgr_image = clip_values(bgr_image, 0, 255).astype(np.uint8)
    return bgr_image
    
# implementation of split
def split_image_channels(image):
    b_ch = image[:,:,0]
    g_ch = image[:,:,1]
    r_ch = image[:,:,2]
    return b_ch, g_ch, r_ch

# implementation of clip
def clip_values(array, min, max):
    clip = np.copy(array)
    # clip values
    clip[clip < min] = min
    clip[clip > max] = max
    return clip

# implementation of sum
def sum_array(array):
    sum = 0
    for i in range(array.shape[0]):
        for j in range(array.shape[1]):
            sum += array[i, j]
    return sum

def CNN_design():
  input = Input(shape=(None, None, 1))
  x = Conv2D(32,3, activation=None, padding='same')(input)
  x = BatchNormalization()(x)
  x = Activation('relu')(x)

  x = Conv2D(64,3, activation=None, padding='same')(x)
  x = BatchNormalization()(x)
  x = Activation('relu')(x)

  x = Conv2D(128,3, activation=None, padding='same')(x)
  x = BatchNormalization()(x)
  x = Activation('relu')(x)

  x = UpSampling2D(2)(x)

  x = Conv2D(64,3, activation=None, padding='same')(x)
  x = BatchNormalization()(x)
  x = Activation('relu')(x)

  x = Conv2D(32,3, activation=None, padding='same')(x)
  x = BatchNormalization()(x)
  x = Activation('relu')(x)

  x = Conv2D(1,3, activation=None, padding='same')(x)
  x = Activation('tanh')(x)
  x = x*127.5 + 127.5

  model = Model([input], x)
  model.summary()
  return model

def pull_database():
  x_input = []
  y_target = []
  for image_in_database in tqdm(glob('/content/DIV2K_train_HR/*.png')):
    image_train = cv2.imread(image_in_database)
    YUVzz_image = convert_RGB2YUV(image_train)
    Y_channel = YUV_image[:,:,0]
    Y_output = cv2.resize(Y_channel, (128, 128), interpolation = cv2.INTER_AREA)
    Y_input = cv2.resize(Y_output, None, fx=0.5, fy=0.5, interpolation = cv2.INTER_AREA)
    x_input.append(Y_input)
    y_target.append(Y_output)
  x_input = np.array(x_input)
  y_target = np.array(y_target)

  return x_input, y_target

train_model = CNN_design()

X_in, Y_pred = pull_database()
print(X_in.shape, Y_pred.shape)

import matplotlib.pyplot as plt

plt.subplot(211)
plt.imshow(X_in[0], cmap='gray')
plt.subplot(212)
plt.imshow(Y_pred[0], cmap='gray')
plt.show()

from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping
import cv2


x_training, x_validation, y_training, y_validation = train_test_split(X_in, Y_pred, test_size=0.2, random_state = 7623)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
mse_loss = 'mse'
train_model.compile(loss=mse_loss, optimizer=optimizer)

callbk_save_model = tf.keras.callbacks.ModelCheckpoint(
    'drive/MyDrive/training/model/2x_upsample_model.h5',
    monitor = 'val_loss',
    verbose=1,
    save_best_only=True,
    mode='min',
    save_freq='epoch'
)
error_callbk = tf.keras.callbacks.TensorBoard(log_dir='./drive/MyDrive/training/graph/2x_upsample/', histogram_freq=0, write_graph=True, write_images=True)
callbk_earlyStop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)


batch_size = 4
epochs = 100
train_model.fit(x_training, y_training, validation_data=( x_validation,y_validation), batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[error_callbk, callbk_save_model, callbk_earlyStop])


train_model = load_model('drive/MyDrive/training/model/2x_upsample_model.h5')
img = cv2.imread("drive/MyDrive/training/test_image.png")
img_yuv = convert_RGB2YUV(img)
Y_downsampled, U_downsampled, V_downsampled = YUV_downsampling(img_yuv)
Y_pred = np.expand_dims(Y_downsampled,axis=0)
y_upsampled = train_model.predict(Y_pred)
print("downsampled:",Y_downsampled.shape)
print("upsampled:",y_upsampled.shape)
plt.subplot(221)
plt.imshow(Y_downsampled, cmap='gray')
plt.subplot(222)
plt.imshow(y_upsampled[0], cmap='gray')
plt.subplot(223)
plt.imshow(img_yuv[:,:,0], cmap='gray')
plt.show()